# -*- coding: utf-8 -*-
"""mt5 summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TfxfzPsXaBMq0Nd8SfIpTrjbw_0q39Fr
"""


from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset

english_dataset.set_format('pandas')

english_df = english_dataset['train']

english_df['product_category'].value_counts()[:20]

english_dataset.reset_format()

english_dataset = english_dataset.filter(lambda x: x['product_category'] == 'book' or x['product_category'] == 'digital_ebook_purchase')
spanish_dataset = spanish_dataset.filter(lambda x: x['product_category'] == 'book' or x['product_category'] == 'digital_ebook_purchase')

from datasets import concatenate_datasets, DatasetDict

dataset = DatasetDict()
for key in english_dataset.keys():
  dataset[key] = (concatenate_datasets([english_dataset[key], spanish_dataset[key]])).shuffle()

dataset

dataset = dataset.filter(lambda x: len(x["review_title"].split()) > 2)

from transformers import AutoTokenizer

checkpoint = 'google/mt5-small'

tokenizer = AutoTokenizer.from_pretrained(checkpoint)

inputs = tokenizer('I loved reading the Hunger Games!')
inputs

print(tokenizer.decode(inputs['input_ids'], skip_special_tokens=False))
print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))

max_input_length = 512
max_target_length = 30

def preprocess_inputs(examples):
  model_inputs = tokenizer(examples['review_body'], max_length = max_input_length, truncation=True)
  

  with tokenizer.as_target_tokenizer():
    labels = tokenizer(examples['review_title'], max_length = max_target_length, truncation=True)
  
  model_inputs['labels'] = labels['input_ids']
  return model_inputs

tokenized_dataset = dataset.map(preprocess_inputs, batched=True)

tokenized_dataset


from datasets import load_metric
rouge_score = load_metric('rouge')

"""### Creating a strong baselines (LEAD-3)"""


import nltk
nltk.download('punkt')

from nltk.tokenize import sent_tokenize

def lead_sentence_extractor(text, n = 3): 
  return '\n'.join(sent_tokenize(text)[:n])

print(lead_sentence_extractor(tokenized_dataset["train"][13]["review_body"]))

def evaluate_baseline(tokenized_dataset, metric):
  summary = [lead_sentence_extractor(text) for text in tokenized_dataset['review_body']]
  return metric.compute(predictions = summary, references = tokenized_dataset['review_title'])

import pandas as pd

score = evaluate_baseline(tokenized_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict

from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_training_epochs = 8

logging_steps = len(tokenized_dataset['train']) // batch_size

args = Seq2SeqTrainingArguments(
    output_dir='summarization_t5',
    evaluation_strategy='steps', 
    do_eval=True,
    do_train = True,
    num_train_epochs=num_training_epochs,
    eval_steps=200,
    weight_decay = 0.01,
    predict_with_generate=True,
    logging_steps=logging_steps,
    learning_rate=5.6e-5,
    save_total_limit = 3,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
)

import numpy as np
def compute_metrics(eval_pred):
  predictions, labels = eval_pred

  predictions_str = tokenizer.batch_decode(predictions, skip_special_tokens = True)
  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

  labels_str = tokenizer.batch_decode(labels, skip_special_tokens = True)
  predictions_str = ["\n".join(sent_tokenize(pred.strip())) for pred in predictions_str]
  labels_str = ["\n".join(sent_tokenize(label.strip())) for label in labels_str]

  result = rouge_score.compute(predictions = predictions_str, references = labels_str, use_stemmer = True)

  return {
      key: round(value.mid.fmeasure * 100, 4) for key, value in result.items()
  }

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model = model)

tokenized_dataset = tokenized_dataset.remove_columns(dataset['train'].column_names)

batch = [tokenized_dataset['train'][i] for i in range(2)]
data_collator(batch)

from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

